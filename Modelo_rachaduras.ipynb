{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6f3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4beb89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ester\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df49cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_train_crack = 'dataset/train/crack/'\n",
    "base_dir_train_non_crack = 'dataset/train/non-crack-subset/'\n",
    "base_dir_test_crack = 'dataset/test/crack/'\n",
    "base_dir_test_non_crack = 'dataset/test/non-crack-subset/'\n",
    "base_dir_processed_train_images = 'processed-dataset-train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fbf5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_image_name_prefix = 'preprocessed_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127ba43",
   "metadata": {},
   "source": [
    "Obtendo listas com o nome das imagens de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "712e1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_images_for_train_name_list = os.listdir(base_dir_train_crack)\n",
    "non_crack_images_for_train_name_list = os.listdir(base_dir_train_non_crack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0b59917",
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_images_for_test_name_list = os.listdir(base_dir_test_crack)\n",
    "non_crack_images_for_test_name_list = os.listdir(base_dir_test_non_crack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2034da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_name_list(base_dir, list_with_names, default_image_width, default_image_height, cv2_imread_colorscheme):\n",
    "    loaded_images = []\n",
    "    for image_name in list_with_names:\n",
    "        im = cv2.imread(base_dir + image_name, cv2_imread_colorscheme)\n",
    "        im_resized = cv2.resize(im, (default_image_width, default_image_height))\n",
    "        loaded_images.append(im_resized)\n",
    "    return loaded_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31105684",
   "metadata": {},
   "source": [
    "Definindo constantes (tamanho das imagens e esquema de cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03aa6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_image_width = 256\n",
    "default_image_height = 256\n",
    "default_colorscheme = cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2f3fa",
   "metadata": {},
   "source": [
    "Carregando as imagens para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85339a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_images_for_train_list = load_images_from_name_list(base_dir_train_crack, \\\n",
    "                                                         crack_images_for_train_name_list, \\\n",
    "                                                         default_image_width, default_image_height, \\\n",
    "                                                         default_colorscheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec742c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_crack_images_for_train_list = load_images_from_name_list(base_dir_train_non_crack, \\\n",
    "                                                             non_crack_images_for_train_name_list, \\\n",
    "                                                             default_image_width, default_image_height, \\\n",
    "                                                             default_colorscheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5b41068",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_blur_kernel_size = 3\n",
    "clahe_clip_limit = 4.0\n",
    "clahe_tile_grid_size = (30, 30)\n",
    "first_erosion_iterations = 3\n",
    "first_dilation_iterations = 2\n",
    "size_for_erosion_and_dilation_element = 2\n",
    "\n",
    "element_for_erosion_and_dilation = \\\n",
    "    cv2.getStructuringElement(cv2.MORPH_CROSS, \\\n",
    "                              (2 * size_for_erosion_and_dilation_element + 1, 2 * size_for_erosion_and_dilation_element + 1), \\\n",
    "                              (size_for_erosion_and_dilation_element, size_for_erosion_and_dilation_element))\n",
    "\n",
    "addapt_thresh_neighbour_size = 11\n",
    "addapt_thresh_subtraction_constant = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae370a01",
   "metadata": {},
   "source": [
    "Função que aplica o pré processamento nas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32f3bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_images_from_list(images_list, \\\n",
    "                                 median_blur_kernel_size, \\\n",
    "                                 clahe_clip_limit, \\\n",
    "                                 clahe_tile_grid_size, \\\n",
    "                                 first_erosion_iterations, \\\n",
    "                                 element_for_erosion_and_dilation, \\\n",
    "                                 first_dilation_iterations, \\\n",
    "                                 addapt_thresh_neighbour_size, addapt_thresh_subtraction_constant):\n",
    "    \n",
    "    pre_processed_images = []\n",
    "    clahe = cv2.createCLAHE(clipLimit = clahe_clip_limit, tileGridSize = clahe_tile_grid_size)\n",
    "    \n",
    "    for image in images_list:\n",
    "        blured_image = cv2.medianBlur(image, median_blur_kernel_size) # aplica filtro de mediana\n",
    "        clahe_image = clahe.apply(blured_image) # aplica equalizacao de histograma por blocos, uniformiza iluminacao\n",
    "        min_pixel_value = np.min(clahe_image) # obtem o pixel de menor valor na imagem\n",
    "        darker_image = clahe_image - min_pixel_value # escurece toda a imagem, subtraindo o valor do menor pixel\n",
    "        \n",
    "        erosion_image = cv2.erode(darker_image, \\\n",
    "                                  element_for_erosion_and_dilation, \\\n",
    "                                  iterations = first_erosion_iterations) # aplica 2 erosoes\n",
    "        \n",
    "        dilated_image = cv2.dilate(erosion_image, \\\n",
    "                                   element_for_erosion_and_dilation, \\\n",
    "                                   iterations = first_dilation_iterations) # aplica 2 dilatacoes\n",
    "        \n",
    "        image_threshold = cv2.adaptiveThreshold(dilated_image, 255, \\\n",
    "                                                cv2.ADAPTIVE_THRESH_MEAN_C, \\\n",
    "                                                cv2.THRESH_BINARY, \\\n",
    "                                                addapt_thresh_neighbour_size, \\\n",
    "                                                addapt_thresh_subtraction_constant)\n",
    "        \n",
    "        #image_negative = cv2.bitwise_not(image_threshold)\n",
    "        pre_processed_images.append(image_threshold)\n",
    "        \n",
    "    return pre_processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a23eb7",
   "metadata": {},
   "source": [
    "Pré-processando as imagens para treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dec60912",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_train_crack = pre_process_images_from_list(crack_images_for_train_list, \\\n",
    "                                                         median_blur_kernel_size, \\\n",
    "                                                         clahe_clip_limit, \\\n",
    "                                                         clahe_tile_grid_size, \\\n",
    "                                                         first_erosion_iterations, \\\n",
    "                                                         element_for_erosion_and_dilation, \\\n",
    "                                                         first_dilation_iterations, \\\n",
    "                                                         addapt_thresh_neighbour_size, \\\n",
    "                                                         addapt_thresh_subtraction_constant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b362a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_train_non_crack = pre_process_images_from_list(non_crack_images_for_train_list, \\\n",
    "                                                         median_blur_kernel_size, \\\n",
    "                                                         clahe_clip_limit, \\\n",
    "                                                         clahe_tile_grid_size, \\\n",
    "                                                         first_erosion_iterations, \\\n",
    "                                                         element_for_erosion_and_dilation, \\\n",
    "                                                         first_dilation_iterations, \\\n",
    "                                                         addapt_thresh_neighbour_size, \\\n",
    "                                                         addapt_thresh_subtraction_constant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65df93f",
   "metadata": {},
   "source": [
    "Função para salvar imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64f3f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(base_dir, images_list, images_names_list, image_name_prefix, directory_to_save):\n",
    "    for idx in range(len(images_names_list)):\n",
    "        \n",
    "        if not os.path.exists(base_dir + directory_to_save):\n",
    "            os.makedirs(base_dir + directory_to_save)\n",
    "            \n",
    "        cv2.imwrite(base_dir + directory_to_save + '/' + image_name_prefix + images_names_list[idx], images_list[idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853433e8",
   "metadata": {},
   "source": [
    "Salvando as imagens de treinamento pré-processadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1f585dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(base_dir_processed_train_images, \\\n",
    "            pre_processed_train_crack, \\\n",
    "            crack_images_for_train_name_list, \\\n",
    "            preprocessed_image_name_prefix, \\\n",
    "            'crack')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ac4fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(base_dir_processed_train_images, \\\n",
    "            pre_processed_train_non_crack, \\\n",
    "            non_crack_images_for_train_name_list, \\\n",
    "            preprocessed_image_name_prefix, \\\n",
    "            'non-crack')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027769a",
   "metadata": {},
   "source": [
    "Treinando modelo Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcd28a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_input_masks = []\n",
    "svm_input_masks.extend(pre_processed_train_crack)\n",
    "svm_input_masks.extend(pre_processed_train_non_crack) # adicionamos todas as imagens à lista de input do modelo\n",
    "svm_input_labels = []\n",
    "svm_input_labels.extend(np.ones((len(pre_processed_train_crack), ), np.uint8))\n",
    "svm_input_labels.extend(np.zeros((len(pre_processed_train_non_crack), ), np.uint8)) # adicionamos as labels das imagens\n",
    "\n",
    "svm_input_masks = list(map(lambda x:x.flatten(), svm_input_masks)) # aqui, redimensionamos as imagens para terem 1 dimensao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37ccde76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(svm_input_masks, svm_input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b88ced9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_images_for_test_list = load_images_from_name_list(base_dir_test_crack, \\\n",
    "                                                        crack_images_for_test_name_list, \\\n",
    "                                                        default_image_width, default_image_height, \\\n",
    "                                                        default_colorscheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "871794bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_crack_images_for_test_list = load_images_from_name_list(base_dir_test_non_crack, \\\n",
    "                                                            non_crack_images_for_test_name_list, \\\n",
    "                                                            default_image_width, default_image_height, \\\n",
    "                                                            default_colorscheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7fb386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_crack = pre_process_images_from_list(crack_images_for_test_list, \\\n",
    "                                                        median_blur_kernel_size, \\\n",
    "                                                        clahe_clip_limit, \\\n",
    "                                                        clahe_tile_grid_size, \\\n",
    "                                                        first_erosion_iterations, \\\n",
    "                                                        element_for_erosion_and_dilation, \\\n",
    "                                                        first_dilation_iterations, \\\n",
    "                                                        addapt_thresh_neighbour_size, \\\n",
    "                                                        addapt_thresh_subtraction_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d0ef35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_non_crack = pre_process_images_from_list(non_crack_images_for_test_list, \\\n",
    "                                                            median_blur_kernel_size, \\\n",
    "                                                            clahe_clip_limit, \\\n",
    "                                                            clahe_tile_grid_size, \\\n",
    "                                                            first_erosion_iterations, \\\n",
    "                                                            element_for_erosion_and_dilation, \\\n",
    "                                                            first_dilation_iterations, \\\n",
    "                                                            addapt_thresh_neighbour_size, \\\n",
    "                                                            addapt_thresh_subtraction_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ba3a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_test_masks = []\n",
    "svm_test_masks.extend(pre_processed_test_crack)\n",
    "svm_test_masks.extend(pre_processed_test_non_crack)\n",
    "svm_test_masks = list(map(lambda x:x.flatten(), svm_test_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53a60b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_test_labels = []\n",
    "svm_test_labels.extend(np.ones((len(pre_processed_test_crack), ), np.uint8))\n",
    "svm_test_labels.extend(np.zeros((len(pre_processed_test_non_crack), ), np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30a72581",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_test_images_prediction = svm_classifier.predict(svm_test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28773665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178343949044586"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(svm_test_images_prediction, svm_test_labels)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc9ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2eaac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5afd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b2f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e3b3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_images_for_test_list = []\n",
    "for image_name in crack_images_for_test_name_list:\n",
    "    im = cv2.imread('dataset/test/crack/'+ image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    im_resized = cv2.resize(im, (image_width, image_heigth))\n",
    "    im_blur = cv2.medianBlur(im_resized, 3)\n",
    "    crack_images_for_test_list.append(im_blur)\n",
    "    \n",
    "noncrack_images_for_test_list = []\n",
    "for image_name in noncrack_images_for_test_name_list:\n",
    "    im = cv2.imread('dataset/test/non-crack(vamos_usar)/'+ image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    im_resized = cv2.resize(im, (image_width, image_heigth))\n",
    "    im_blur = cv2.medianBlur(im_resized, 3)\n",
    "    noncrack_images_for_test_list.append(im_blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07c2b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(30,30))\n",
    "\n",
    "test_posprocessed_crack = []\n",
    "for image in crack_images_for_test_list:\n",
    "    clahe_image = clahe.apply(image)\n",
    "    min_pixel_value = np.min(clahe_image)\n",
    "    darker_image = clahe_image - min_pixel_value\n",
    "    erosion_image = cv2.erode(darker_image, element, iterations = 3)\n",
    "    dilated_image = cv2.dilate(erosion_image, element, iterations = 2) # era 2 iteracoes\n",
    "    image_threshold = cv2.adaptiveThreshold(dilated_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    threshold_not = cv2.bitwise_not(image_threshold)\n",
    "    test_posprocessed_crack.append(threshold_not)\n",
    "\n",
    "test_posprocessed_non_crack = []\n",
    "for image in noncrack_images_for_test_list:\n",
    "    clahe_image = clahe.apply(image)\n",
    "    min_pixel_value = np.min(clahe_image)\n",
    "    darker_image = clahe_image - min_pixel_value\n",
    "    erosion_image = cv2.erode(darker_image, element, iterations = 3)\n",
    "    dilated_image = cv2.dilate(erosion_image, element, iterations = 2) # era 2 iteracoes\n",
    "    image_threshold = cv2.adaptiveThreshold(dilated_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    threshold_not = cv2.bitwise_not(image_threshold)\n",
    "    test_posprocessed_non_crack.append(threshold_not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6635d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_images_list = []\n",
    "for image_name in crack_images_name_list:\n",
    "    im = cv2.imread('dataset/train/crack/'+ image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    im_resized = cv2.resize(im, (image_width, image_heigth))\n",
    "    im_blur = cv2.medianBlur(im_resized, 3)\n",
    "    crack_images_list.append(im_blur)\n",
    "    \n",
    "noncrack_images_list = []\n",
    "for image_name in noncrack_images_name_list:\n",
    "    im = cv2.imread('dataset/train/non-crack(vamos_usar)/'+ image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    im_resized = cv2.resize(im, (image_width, image_heigth))\n",
    "    im_blur = cv2.medianBlur(im_resized, 3)\n",
    "    noncrack_images_list.append(im_blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2687166",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 256\n",
    "image_heigth = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614d8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_images_list = []\n",
    "for image_name in crack_images_name_list:\n",
    "    im = cv2.imread('dataset/train/crack/'+ image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    im_resized = cv2.resize(im, (image_width, image_heigth))\n",
    "    im_blur = cv2.medianBlur(im_resized, 3)\n",
    "    crack_images_list.append(im_blur)\n",
    "    \n",
    "noncrack_images_list = []\n",
    "for image_name in noncrack_images_name_list:\n",
    "    im = cv2.imread('dataset/train/non-crack(vamos_usar)/'+ image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    im_resized = cv2.resize(im, (image_width, image_heigth))\n",
    "    im_blur = cv2.medianBlur(im_resized, 3)\n",
    "    noncrack_images_list.append(im_blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c930b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(crack_images_list[210], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb7b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos\n",
    "clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(30,30))\n",
    "\n",
    "clahe_crack = []\n",
    "for image in crack_images_list:\n",
    "    clahe_crack.append(clahe.apply(image))\n",
    "\n",
    "clahe_non_crack = []\n",
    "for image in noncrack_images_list:\n",
    "    clahe_non_crack.append(clahe.apply(image))\n",
    "\n",
    "#save_single_channel_processed_images_for_comparison(crack_images_list, \\\n",
    "#                                                    crack_images_name_list, \\\n",
    "#                                                    clahe_processed, \\\n",
    "#                                                    'clahe')\n",
    "\n",
    "#image_test_clahe = crack_images_list[210].copy()\n",
    "#enhanced_img = clahe.apply(image_test_clahe)\n",
    "#enhanced_img = cv2.cvtColor(image_test_clahe, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "#result = np.hstack((image_test_clahe, enhanced_img))\n",
    "#cv2.imshow('Result', result)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad72d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nao usamos\n",
    "thresholded_clahe_crack_images = []\n",
    "for image in clahe_processed:\n",
    "    ret, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    thresholded_clahe_crack_images.append(thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d4badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nao usamos\n",
    "save_single_channel_processed_images_for_comparison(clahe_processed, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    thresholded_clahe_crack_images, \\\n",
    "                                                    'clahe-otsu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e9370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos\n",
    "darker_clahe_crack = []\n",
    "for image in clahe_crack:\n",
    "    min_pixel_value = np.min(image)\n",
    "    darker_image = image - min_pixel_value\n",
    "    darker_clahe_crack.append(darker_image)\n",
    "    \n",
    "darker_clahe_non_crack = []\n",
    "for image in clahe_non_crack:\n",
    "    min_pixel_value = np.min(image)\n",
    "    darker_image = image - min_pixel_value\n",
    "    darker_clahe_non_crack.append(darker_image)\n",
    "    \n",
    "#save_single_channel_processed_images_for_comparison(clahe_processed, \\\n",
    "#                                                    crack_images_name_list, \\\n",
    "#                                                    darker_clahe, \\\n",
    "#                                                    'clahe-darker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7412fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos\n",
    "erosion_size = 2\n",
    "element = cv2.getStructuringElement(cv2.MORPH_CROSS, (2 * erosion_size + 1, 2 * erosion_size + 1), (erosion_size, erosion_size))\n",
    "\n",
    "eroded_darker_clahe_crack = []\n",
    "for image in darker_clahe_crack:\n",
    "    erosion_dst = cv2.erode(image, element, iterations = 3)\n",
    "    eroded_darker_clahe_crack.append(erosion_dst)\n",
    "    \n",
    "eroded_darker_clahe_non_crack = []\n",
    "for image in darker_clahe_non_crack:\n",
    "    erosion_dst = cv2.erode(image, element, iterations = 3)\n",
    "    eroded_darker_clahe_non_crack.append(erosion_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78bf745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_single_channel_processed_images_for_comparison(darker_clahe, \\\n",
    "#                                                    crack_images_name_list, \\\n",
    "#                                                    eroded_darker_clahe, \\\n",
    "#                                                    'clahe-darker-eroded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d72b619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos\n",
    "dilated_eroded_darker_clahe_crack = []\n",
    "for image in eroded_darker_clahe_crack:\n",
    "    dilated_dst = cv2.dilate(image, element, iterations = 2) # era 2 iteracoes\n",
    "    dilated_eroded_darker_clahe_crack.append(dilated_dst)\n",
    "\n",
    "dilated_eroded_darker_clahe_non_crack = []\n",
    "for image in eroded_darker_clahe_non_crack:\n",
    "    dilated_dst = cv2.dilate(image, element, iterations = 2) # era 2\n",
    "    dilated_eroded_darker_clahe_non_crack.append(dilated_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b960b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nao tinha essa etapa aqui\n",
    "eroded_dilated_eroded_darker_clahe_crack = []\n",
    "for image in dilated_eroded_darker_clahe_crack:\n",
    "    erosion_dst = cv2.erode(image, element, iterations = 2)\n",
    "    eroded_dilated_eroded_darker_clahe_crack.append(erosion_dst)\n",
    "    \n",
    "eroded_dilated_eroded_darker_clahe_non_crack = []\n",
    "for image in dilated_eroded_darker_clahe_non_crack:\n",
    "    erosion_dst = cv2.erode(image, element, iterations = 2)\n",
    "    eroded_dilated_eroded_darker_clahe_non_crack.append(erosion_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a76612e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "gradmorfol_eroded_dilated_eroded_darker_clahe_crack = []\n",
    "for image in eroded_dilated_eroded_darker_clahe_crack:\n",
    "    gradient = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n",
    "    gradmorfol_eroded_dilated_eroded_darker_clahe_crack.append(gradient)\n",
    "\n",
    "gradmorfol_eroded_dilated_eroded_darker_clahe_non_crack = []\n",
    "for image in eroded_dilated_eroded_darker_clahe_non_crack:\n",
    "    gradient = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n",
    "    gradmorfol_eroded_dilated_eroded_darker_clahe_non_crack.append(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4fe17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(eroded_dilated_eroded_darker_clahe_crack, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    gradmorfol_eroded_dilated_eroded_darker_clahe_crack, \\\n",
    "                                                    'gradmorfol_eroded_dilated_eroded_darker_clahe_crack')\n",
    "save_single_channel_processed_images_for_comparison(eroded_dilated_eroded_darker_clahe_non_crack, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    gradmorfol_eroded_dilated_eroded_darker_clahe_non_crack, \\\n",
    "                                                    'gradmorfol_eroded_dilated_eroded_darker_clahe_non_crack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1021107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nao usamos\n",
    "binary_masks_1 = []\n",
    "for image in dilated_eroded_darker_clahe:\n",
    "    threshold_value = 60  # Valor de limiar a ser definido\n",
    "    binary_edges = (image < threshold_value).astype(np.uint8)*255\n",
    "    binary_masks_1.append(binary_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73f2fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nao usamos\n",
    "save_single_channel_processed_images_for_comparison(dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    binary_masks_1, \\\n",
    "                                                    'clahe-darker-eroded-dilated-binary-60')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59b2bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos\n",
    "adaptthresh_dilated_eroded_darker_clahe_crack = []\n",
    "for image in dilated_eroded_darker_clahe_crack:\n",
    "    image_threshold = \\\n",
    "        cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    adaptthresh_dilated_eroded_darker_clahe_crack.append(cv2.bitwise_not(image_threshold))\n",
    "    \n",
    "adaptthresh_dilated_eroded_darker_clahe_non_crack = []\n",
    "for image in dilated_eroded_darker_clahe_non_crack:\n",
    "    image_threshold = \\\n",
    "        cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    adaptthresh_dilated_eroded_darker_clahe_non_crack.append(cv2.bitwise_not(image_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "942d57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(dilated_eroded_darker_clahe_non_crack, \\\n",
    "                                                    noncrack_images_name_list, \\\n",
    "                                                    adaptthresh_dilated_eroded_darker_clahe_non_crack, \\\n",
    "                                                    'adaptthresh_dilated_eroded_darker_clahe_non_crack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52b0d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "eroded_adaptthresh_dilated_eroded_darker_clahe_crack = []\n",
    "for image in adaptthresh_dilated_eroded_darker_clahe_crack:\n",
    "    erosion_dst = cv2.erode(image, element, iterations = 1)\n",
    "    eroded_adaptthresh_dilated_eroded_darker_clahe_crack.append(erosion_dst)\n",
    "    \n",
    "eroded_adaptthresh_dilated_eroded_darker_clahe_non_crack = []\n",
    "for image in adaptthresh_dilated_eroded_darker_clahe_non_crack:\n",
    "    erosion_dst = cv2.erode(image, element, iterations = 1)\n",
    "    eroded_adaptthresh_dilated_eroded_darker_clahe_non_crack.append(erosion_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdce453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos\n",
    "conncomp_eroded_adaptthresh_dilated_eroded_darker_clahe_crack = []\n",
    "for image in eroded_adaptthresh_dilated_eroded_darker_clahe_crack:\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(image)\n",
    "    \n",
    "    areas = [stats[i, cv2.CC_STAT_AREA] for i in range(1, num_labels)]\n",
    "    max_area = np.max(areas) * 0.2\n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] < max_area:\n",
    "            labels[labels == i] = 0\n",
    "    \n",
    "    posproc_image = (labels > 0).astype(np.uint8) * 255\n",
    "    conncomp_eroded_adaptthresh_dilated_eroded_darker_clahe_crack.append(posproc_image)\n",
    "    \n",
    "conncomp_eroded_adaptthresh_dilated_eroded_darker_clahe_non_crack = []\n",
    "for image in eroded_adaptthresh_dilated_eroded_darker_clahe_non_crack:\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(image)\n",
    "    \n",
    "    areas = [stats[i, cv2.CC_STAT_AREA] for i in range(1, num_labels)]\n",
    "    max_area = np.max(areas) * 0.2\n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] < max_area:\n",
    "            labels[labels == i] = 0\n",
    "    \n",
    "    posproc_image = (labels > 0).astype(np.uint8) * 255\n",
    "    conncomp_eroded_adaptthresh_dilated_eroded_darker_clahe_non_crack.append(posproc_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cd95c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(conncomp_eroded_adaptthresh_dilated_eroded_darker_clahe_crack, crack_images_name_list, 'preprocessed_crack_1')\n",
    "save_images(conncomp_eroded_adaptthresh_dilated_eroded_darker_clahe_non_crack, noncrack_images_name_list, 'preprocessed_non_crack_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb6801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "effa496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6aada08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_input_masks_test = []\n",
    "svm_input_masks_test.extend(test_posprocessed_crack)\n",
    "svm_input_masks_test.extend(test_posprocessed_non_crack)\n",
    "svm_input_labels_test = []\n",
    "svm_input_labels_test.extend(np.ones((len(test_posprocessed_crack),), np.uint8))\n",
    "svm_input_labels_test.extend(np.zeros((len(test_posprocessed_non_crack),), np.uint8))\n",
    "\n",
    "svm_input_masks_test = list(map(lambda x:x.flatten(), svm_input_masks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95264d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f44a223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = 'svm_model.pkl'\n",
    "joblib.dump(svm_classifier, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51a83f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9be7c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(svm_input_masks, svm_input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17ea8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_classifier.predict(svm_input_masks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e92bf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5987261146496815"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy2 = accuracy_score(svm_input_labels_test, predictions)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b911e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e067992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_classifier.fit(svm_input_masks, svm_input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ae388516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ester\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "predictions2 = knn_classifier.predict(svm_input_masks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "144d3374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5095541401273885"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy3 = accuracy_score(svm_input_labels_test, predictions2)\n",
    "accuracy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23b87519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOBEL OPERATOR\n",
    "sobel_dilated_eroded_darker_clahe = []\n",
    "for image in dilated_eroded_darker_clahe:\n",
    "    gradient_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    gradient_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "    gradient_magnitude = cv2.convertScaleAbs(gradient_magnitude)\n",
    "    \n",
    "    sobel_dilated_eroded_darker_clahe.append(gradient_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bc38204",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    sobel_dilated_eroded_darker_clahe, \\\n",
    "                                                    'clahe-darker-eroded-dilated-sobel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d07ae492",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_masks_2 = []\n",
    "for image in sobel_dilated_eroded_darker_clahe:\n",
    "    threshold_value = 100  # Valor de limiar a ser definido\n",
    "    binary_edges = (image < threshold_value).astype(np.uint8)*255\n",
    "    binary_masks_2.append(binary_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfa713a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(sobel_dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    binary_masks_2, \\\n",
    "                                                    'clahe-darker-eroded-dilated-sobel-binary-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43ddd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_dilated_eroded_darker_clahe = []\n",
    "for image in dilated_eroded_darker_clahe:\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    laplacian = np.uint8(np.absolute(laplacian))\n",
    "    laplacian_dilated_eroded_darker_clahe.append(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e24266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    laplacian_dilated_eroded_darker_clahe, \\\n",
    "                                                    'clahe-darker-eroded-dilated-laplacian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "610c17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    adaptthresh_dilated_eroded_darker_clahe, \\\n",
    "                                                    'clahe-darker-eroded-dilated-adaptthresh-11-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b2803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eroded_adaptthresh_dilated_eroded_darker_clahe = []\n",
    "erosion_size = 2\n",
    "element = cv2.getStructuringElement(cv2.MORPH_CROSS, (2 * erosion_size + 1, 2 * erosion_size + 1), (erosion_size, erosion_size))\n",
    "for image in adaptthresh_dilated_eroded_darker_clahe:\n",
    "    erosion_dst = cv2.erode(image, element, iterations = 1)\n",
    "    eroded_adaptthresh_dilated_eroded_darker_clahe.append(erosion_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ace105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianfilter_adaptthresh_dilated_eroded_darker_clahe = []\n",
    "for image in adaptthresh_dilated_eroded_darker_clahe:\n",
    "    median = cv2.medianBlur(image, 5)\n",
    "    medianfilter_adaptthresh_dilated_eroded_darker_clahe.append(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ebd4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(adaptthresh_dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    medianfilter_adaptthresh_dilated_eroded_darker_clahe, \\\n",
    "                                                    'clahe-darker-eroded-dilated-adaptthresh-11-2-medianfilter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e6ab6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(eroded_adaptthresh_dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    conncomp_adaptthresh_dilated_eroded_darker_clahe, \\\n",
    "                                                    'clahe-darker-eroded-dilated-adaptthresh-11-2-conncomp-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2abbac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enclosure_adaptthresh_dilated_eroded_darker_clahe = []\n",
    "kernel = np.ones((4, 4), np.uint8)\n",
    "for image in adaptthresh_dilated_eroded_darker_clahe:\n",
    "    imagem_sem_ruido = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "    enclosure_adaptthresh_dilated_eroded_darker_clahe.append(imagem_sem_ruido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f05c1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(adaptthresh_dilated_eroded_darker_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    enclosure_adaptthresh_dilated_eroded_darker_clahe, \\\n",
    "                                                    'clahe-darker-eroded-dilated-adaptthresh-11-2-enclosure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3f16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86e07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331219ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b909e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(sobel_eroded_clahe, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    binary_masks_1, \\\n",
    "                                                    'binary-sobel-eroded-clahe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b0993a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "canny_clahe = []\n",
    "for image_clahe in clahe_processed:\n",
    "    threshold_value, _ = cv2.threshold(image_clahe, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    canny_res = cv2.Canny(image_clahe, np.round(threshold_value * 0.5), np.round(threshold_value))\n",
    "    canny_clahe.append(canny_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "52fd980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(crack_images_list, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    canny_clahe, \\\n",
    "                                                    'canny-clahe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0affe4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', crack_images_list[0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dc155371",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = np.zeros((256, 256, 3)).astype(np.uint8)\n",
    "new_img[:,:,0] = crack_images_list[3][:,:,0]\n",
    "new_img[:,:,1] = (crack_images_list[3][:,:,1]*1.5).astype(np.uint8)\n",
    "new_img[:,:,2] = crack_images_list[3][:,:,2] #cv2.equalizeHist(\n",
    "\n",
    "cv2.imshow('contrasted', cv2.cvtColor(new_img, cv2.COLOR_HSV2RGB))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1502e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_hsi = []\n",
    "for image in crack_images_list:\n",
    "    image_matrix = np.array(image)\n",
    "    h_matrix, s_matrix, i_matrix = converter_RGB_para_HSI(image_matrix)\n",
    "    images_hsi.append([h_matrix, s_matrix, i_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dba58d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "#cv2.imshow('Hue', (images_hsi[3][0] / (2 * np.pi)).astype(np.uint8))\n",
    "#cv2.imshow('Saturation', (images_hsi[3][1] * 255).astype(np.uint8))\n",
    "#cv2.imshow('Intensity', (images_hsi[3][2] * 255).astype(np.uint8))\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "hsi_image = np.zeros((256, 256, 3)).astype(np.uint8)\n",
    "hsi_image[:,:,0] = ((images_hsi[1][0] / (2 * np.pi)) * 255).astype(np.uint8)\n",
    "hsi_image[:,:,1] = (images_hsi[1][1] * 255).astype(np.uint8)\n",
    "hsi_image[:,:,2] = (images_hsi[1][2] * 255).astype(np.uint8)\n",
    "\n",
    "hue = hsi_image[:, :, 0]\n",
    "saturation = hsi_image[:, :, 1]\n",
    "intensity = hsi_image[:, :, 2]\n",
    "\n",
    "contrast_factor = 12  # Adjust this value to control the contrast enhancement\n",
    "enhanced_intensity = np.clip(intensity * contrast_factor, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Create the enhanced HSI image by combining the adjusted intensity with the original hue and saturation\n",
    "enhanced_hsi_image = np.stack((hue, saturation, enhanced_intensity), axis=-1)\n",
    "\n",
    "#enhanced_hsi_image\n",
    "\n",
    "cv2.imshow('hsi', enhanced_hsi_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#axes[0].imshow((images_hsi[1][0] / (2 * np.pi)).astype(np.uint8), cmap='gray')\n",
    "#axes[1].imshow((images_hsi[1][1] * 255).astype(np.uint8), cmap='gray')\n",
    "#axes[2].imshow((images_hsi[1][2] * 255).astype(np.uint8), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f72aef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(crack_images_list[0].shape)\n",
    "new_img = np.zeros((256, 256, 3)).astype(np.uint8)\n",
    "for x in range(image_width):\n",
    "    for y in range(image_heigth):\n",
    "        new_img[x,y,:] = colorsys.rgb_to_hsv(crack_images_list[0][x][y][0],crack_images_list[0][x][y][1],crack_images_list[0][x][y][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40cd18b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2c8b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('hsi', new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d96214a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\thresh.cpp:1555: error: (-2:Unspecified error) in function 'double __cdecl cv::threshold(const class cv::_InputArray &,const class cv::_OutputArray &,double,double,int)'\n> THRESH_OTSU mode:\n>     'src_type == CV_8UC1 || src_type == CV_16UC1'\n> where\n>     'src_type' is 16 (CV_8UC3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14720\\37542376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mthresholded_crack_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcrack_images_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_OTSU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mthresholded_crack_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\thresh.cpp:1555: error: (-2:Unspecified error) in function 'double __cdecl cv::threshold(const class cv::_InputArray &,const class cv::_OutputArray &,double,double,int)'\n> THRESH_OTSU mode:\n>     'src_type == CV_8UC1 || src_type == CV_16UC1'\n> where\n>     'src_type' is 16 (CV_8UC3)\n"
     ]
    }
   ],
   "source": [
    "# OTSU THRESHOLD\n",
    "thresholded_crack_images = []\n",
    "for image in crack_images_list:\n",
    "    ret, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    thresholded_crack_images.append(thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baf92cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_channel_processed_images_for_comparison(original_images_list, original_images_names_list, \\\n",
    "                                                        processed_images_with_single_channel, \\\n",
    "                                                        directory_to_save):\n",
    "    height = original_images_list[0].shape[0]\n",
    "    width = original_images_list[0].shape[1]\n",
    "    \n",
    "    for idx in range(len(original_images_names_list)):\n",
    "        \n",
    "        collage_width = original_images_list[idx].shape[1] * 2\n",
    "        collage = np.ones((height, collage_width), np.uint8) * 255\n",
    "        collage = cv2.resize(collage, (collage_width, height))\n",
    "        \n",
    "        collage[:, :original_images_list[idx].shape[1]] = original_images_list[idx]\n",
    "        collage[:, original_images_list[idx].shape[1]:] = processed_images_with_single_channel[idx]\n",
    "        \n",
    "        if not os.path.exists('processed-dataset-tests/' + directory_to_save):\n",
    "            os.makedirs('processed-dataset-tests/' + directory_to_save)\n",
    "        cv2.imwrite('processed-dataset-tests/' + directory_to_save + '/' + original_images_names_list[idx], collage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d8244c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(crack_images_list, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    thresholded_crack_images, \\\n",
    "                                                    'only-otsu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d130c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOBEL OPERATOR\n",
    "sobel_images = []\n",
    "canny_images = []\n",
    "for image in crack_images_list:\n",
    "    gradient_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    gradient_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "    gradient_magnitude = cv2.convertScaleAbs(gradient_magnitude)\n",
    "    \n",
    "    sobel_images.append(gradient_magnitude)\n",
    "    \n",
    "    canny_edges = cv2.Canny(gradient_magnitude, threshold1=50, threshold2=150)\n",
    "    \n",
    "    canny_images.append(canny_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc210048",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(crack_images_list, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    sobel_images, \\\n",
    "                                                    'only-sobel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a33346a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(crack_images_list, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    canny_images, \\\n",
    "                                                    'only-canny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71f30b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOBEL + OTSU THRESHOLD\n",
    "sobel_otsu_crack_images = []\n",
    "for image in sobel_images:\n",
    "    ret, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    sobel_otsu_crack_images.append(thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68d55471",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_channel_processed_images_for_comparison(crack_images_list, \\\n",
    "                                                    crack_images_name_list, \\\n",
    "                                                    sobel_otsu_crack_images, \\\n",
    "                                                    'sobel-and-otsu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f6972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
